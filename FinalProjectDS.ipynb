{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa15bbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonmork/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting csv...\n",
      "Getting headers..\n",
      "Cleaning...\n",
      "25537\n",
      "Tokenizing and removing stopwords...\n",
      "16366\n",
      "0.35912597407682967\n",
      "Stemming\n",
      "11006\n",
      "0.3275082488085055\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Calls the clean function from cleantext clean on a string\"\"\"\n",
    "    t = clean(text,\n",
    "    fix_unicode=False,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,         # fully strip line breaks as opposed to only normalizing them NOT WORKING?\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"URL\",\n",
    "    replace_with_email=\"EMAIL\",\n",
    "    replace_with_phone_number=\"PHONE\",\n",
    "    replace_with_number=\"NUMBER\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"CUR\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    return t\n",
    "\n",
    "\n",
    "print(\"Getting csv...\")\n",
    "pd.set_option(\"display.max_colwidth\", 10000000)\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "\n",
    "print(\"Getting headers..\")\n",
    "column_names = list(data.columns.values)\n",
    "\n",
    "\n",
    "print(\"Cleaning...\")\n",
    "for index, row in data.iterrows():\n",
    "    cleaned = clean_text(row['content'])\n",
    "    data.at[index,'content'] = cleaned\n",
    "    \n",
    "    \n",
    "# Split the content column into a list of words\n",
    "words = data['content'].str.split()\n",
    "# Create a set of unique words\n",
    "unique_words = set(word for word_list in words for word in word_list)\n",
    "# Count the number of unique words\n",
    "num_unique_words_after_clean = len(unique_words)\n",
    "print(num_unique_words_after_clean) \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Tokenizing and removing stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for index, row in data.iterrows():    \n",
    "    data.at[index,'content'] = nltk.word_tokenize(row['content'])\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in data.at[index,'content']:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    data.at[index,'content'] = filtered_sentence\n",
    " \n",
    "\n",
    "# concatenate all the lists into a single list of words\n",
    "words = [word for content in data['content'] for word in content]\n",
    "# get the set of unique words\n",
    "unique_words = set(words)\n",
    "# get the number of unique words\n",
    "num_unique_words_after_stop = len(unique_words)\n",
    "print(num_unique_words_after_stop)\n",
    "    \n",
    "print(1-num_unique_words_after_stop/num_unique_words_after_clean)\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"Stemming\")\n",
    "snowball = SnowballStemmer(language='english')\n",
    "for index, row in data.iterrows():   \n",
    "    stemmed = []\n",
    "    for word in data.at[index,'content']:\n",
    "        stemmed.append(snowball.stem(word))\n",
    "    data.at[index,'content'] = stemmed\n",
    "\n",
    "\n",
    "# concatenate all the lists into a single list of words\n",
    "words = [word for content in data['content'] for word in content]\n",
    "# get the set of unique words\n",
    "unique_words = set(words)\n",
    "# get the number of unique words\n",
    "num_unique_words_after_stem = len(unique_words)\n",
    "print(num_unique_words_after_stem)\n",
    "\n",
    "print(1-num_unique_words_after_stem/num_unique_words_after_stop)\n",
    "\n",
    "\n",
    "print(\"Done...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1080e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27842c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tips: nearest neighbor on missing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
