{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bb0c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonmork/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import zipfile\n",
    "import io\n",
    "import csv\n",
    "import zipfile38 as zipfile\n",
    "from dask import dataframe as dd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfa15bbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Define settings for cleaning\n",
    "def clean_text(text):\n",
    "    \"\"\"Calls the clean function from cleantext clean on a string\"\"\"\n",
    "    t = clean(text,\n",
    "    fix_unicode=False,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,         # fully strip line breaks as opposed to only normalizing them NOT WORKING?\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"URL\",\n",
    "    replace_with_email=\"EMAIL\",\n",
    "    replace_with_phone_number=\"PHONE\",\n",
    "    replace_with_number=\"NUM\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"CUR\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    return t\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 10000000)    \n",
    "print(\"Done\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b27fedea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting csv...\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#Small df\n",
    "print(\"Getting csv...\")\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c877a920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len:  2\n",
      "Unnamed: 0                                                                                                                                                                                                                                                                                                                                                                         1\n",
      "id                                                                                                                                                                                                                                                                                                                                                                                 6\n",
      "domain                                                                                                                                                                                                                                                                                                                                                            barenakedislam.com\n",
      "type                                                                                                                                                                                                                                                                                                                                                                            hate\n",
      "url                                                                                                                                                                                                                                                                                                                         http://barenakedislam.com/category/donald-trump/page/43/\n",
      "content             Unfortunately, he hasn’t yet attacked her for Islamic terrorism-connections, but we know he will. Below are several links that explain in detail why having Huma Abedin anywhere near the White House would be like inviting in the terrorism-linked Muslim Brotherhood, which has been banned in Egypt and the UAE. BNI Readers, please forward the damning […]\n",
      "scraped_at                                                                                                                                                                                                                                                                                                                                                2018-01-25 16:17:44.789555\n",
      "inserted_at                                                                                                                                                                                                                                                                                                                                               2018-02-02 01:19:41.756632\n",
      "updated_at                                                                                                                                                                                                                                                                                                                                                2018-02-02 01:19:41.756664\n",
      "title                                                                                                                                                                                                                                                                                                                                                                   Donald Trump\n",
      "authors                                                                                                                                                                                                                                                                                    Linda Rivera, Conrad Calvano, Az Gal, Lincoln Applegate Hahn, Kevin Collins, Jesus Rivera\n",
      "keywords                                                                                                                                                                                                                                                                                                                                                                         NaN\n",
      "meta_keywords                                                                                                                                                                                                                                                                                                                                                                   ['']\n",
      "meta_description                                                                                                                                                                                                                                                                                                                                                                 NaN\n",
      "tags                                                                                                                                                                                                                                                                                                                                                                             NaN\n",
      "summary                                                                                                                                                                                                                                                                                                                                                                          NaN\n",
      "source                                                                                                                                                                                                                                                                                                                                                                           NaN\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "csv_textfilereader = pd.read_csv('/Users/simonmork/Desktop/GitHub/FinalProjectDS/news_cleaned_2018_02_13.csv', chunksize=1000000, iterator=True)\n",
    "\n",
    "first_chunk = csv_textfilereader.get_chunk(2)\n",
    "first_article = first_chunk.iloc[1]\n",
    "print(\"len: \", len(first_chunk))\n",
    "print( first_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7fcb94a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                                                                                                                                                                                                                            4\n",
      "id                                                                                                                                                                                                                                    9\n",
      "domain                                                                                                                                                                                                               barenakedislam.com\n",
      "type                                                                                                                                                                                                                               hate\n",
      "url                                                                                                    http://barenakedislam.com/2017/12/25/oh-trump-you-coward-you-just-wait-we-will-dig-your-grave-by-means-of-the-islamic-caliphate/\n",
      "content             “The time has come to cut off the tongues of those who support peace and those who mourn it.” (Can’t disagree there, Arab Muslims occupying Israel will never live in peace with the native population – the Jews).\n",
      "scraped_at                                                                                                                                                                                                   2018-01-25 16:17:44.789555\n",
      "inserted_at                                                                                                                                                                                                  2018-02-02 01:19:41.756632\n",
      "updated_at                                                                                                                                                                                                   2018-02-02 01:19:41.756664\n",
      "title                                                                                                                                  “Oh, Trump, you coward, you just wait, we will dig your grave by means of the Islamic Caliphate”\n",
      "authors                                                                                        F.N. Lehner, Don Spilman, Clarence J. Feinour, Linda Rivera, Conrad Calvano, Az Gal, Lincoln Applegate Hahn, Kevin Collins, Jesus Rivera\n",
      "keywords                                                                                                                                                                                                                            NaN\n",
      "meta_keywords                                                                                                                                                                                                                      ['']\n",
      "meta_description                                                                                                                                                                                                                    NaN\n",
      "tags                                                                                                                                                                                                                                NaN\n",
      "summary                                                                                                                                                                                                                             NaN\n",
      "source                                                                                                                                                                                                                              NaN\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "first_article = first_chunk.iloc[4]\n",
    "print(first_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2af9eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  id              domain   type  \\\n",
      "0           0   2       express.co.uk  rumor   \n",
      "1           1   6  barenakedislam.com   hate   \n",
      "2           2   7  barenakedislam.com   hate   \n",
      "3           3   8  barenakedislam.com   hate   \n",
      "\n",
      "                                                                                                                                                                                                                url  \\\n",
      "0                                                                                                https://www.express.co.uk/news/science/738402/life-an-ILLUSION-reality-does-not-exist-if-you-are-not-looking-at-it   \n",
      "1                                                                                                                                                          http://barenakedislam.com/category/donald-trump/page/43/   \n",
      "2                                                                                                                                                           http://barenakedislam.com/category/donald-trump/page/2/   \n",
      "3  http://barenakedislam.com/2017/12/24/more-winning-israeli-intelligence-source-debkafile-confirms-the-trump-administration-to-cut-off-ties-with-the-palestinians-which-means-no-peace-plan-no-more-financial-aid/   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              content  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Life is an illusion, at least on a quantum level, in a theory which has recently been confirmed by a set of researchers.\\n\\nThey finally have the means to test John Wheeler’s delayed-choice theory and concluded that the physicist was right.\\n\\nIn 1978, Mr Wheeler’s proposed experiment involved a moving object that was given the choice to act like a wave or a particle – the former acting as a vibration with a frequency that can distinguish it from other waves and the latter having no frequency that you can determine its position in space, unlike a wave – and at what point does it ‘decide’ to act like one or the other.\\n\\nAt the time, the technology was not available to conduct a strong experiment, but scientists have now been able to carry it out.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Unfortunately, he hasn’t yet attacked her for Islamic terrorism-connections, but we know he will. Below are several links that explain in detail why having Huma Abedin anywhere near the White House would be like inviting in the terrorism-linked Muslim Brotherhood, which has been banned in Egypt and the UAE. BNI Readers, please forward the damning […]   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The Los Angeles Police Department has been denied $3 million in federal aid for law enforcement. While there is no official announcement as to why, it is more than likely that it has everything to do with LA’s “sanctuary city” status for harboring illegal aliens. Donald Trump and Attorney General Jeff Sessions have repeatedly said […]   \n",
      "3  The White House has decided to quietly withdraw from all its ties with the Ramallah-based Palestinian Authority and Mahmoud Abbas.\\n\\n\\n\\nDebka (h/t Marvin W) DEBKAfile’s exclusive sources report that the Trump administration has resolved to scrap all ties with the Palestinian leadership in retaliation for its campaign against US President Donald Trump and his Jerusalem policy. Several warnings to Mahmoud Abbas (aka Abu Mazen) of what was in store if he did not desist from castigating the US president fell on death ears.\\n\\nLast week, two Arab crown princes, Saudi Muhammed bin Salman and UAE Sheikh Muhammed bin Zayed, summoned Abbas to their capitals and urged him strongly to back away from his attacks on President Trump. He got the same advice from the ruler of Qatar, Sheikh Tamim Al Thani, who conferred with Washington on the subject – all to no avail. The Trump administration has therefore set out an eight-point program of sanctions, first revealed here:\\n\\nThe Israeli-Palestinian peace plan under preparation in Washington will not be submitted to Ramallah – only to Israel and the relevant Arab governments.\\n\\nUS-Palestinian interaction is to be suspended – not just at the senior levels but in day-to-day interchanges. The administration has notified Palestinian and other Arab parties to stop addressing queries on political and economic matters to the US consulate in Jerusalem, because they will not receive answers.\\n\\nThe status of the PLO office in Washington will be reevaluated with a view to shutting it down.\\n\\nPalestinian officials will no longer be invited to Washington by the US government, including the State Department and Department of Treasury.\\n\\nAbove all, they will not be welcome at the White House or the National Security Council where US Middle East policy is designed. Senior US officials congratulated the senior Palestinian negotiator Saab Erekat, who also holds the PA’s American portfolio, on his recovery from illness, at the same time warning him that he would no longer be received at the White House.\\n\\nThe Trump administration will not make any public announcement of the cutoff of financial aid to the Palestinians. Since the funds are mostly earmarked for specific economic projects, each allocation will simply be held back on the pretext of the need for a “reappraisal.”\\n\\nThe US will halt its contributions to the UN Work and Relief Organization, an estimated one billion dollars per annum.\\n\\nThe US administration moreover intervened with the governments of Saudi Arabia, Abu Dhabi and Qatar with a request that they freeze or slow their economic aid to the Palestinian Authority.\\n\\nAccording DEBKAfile’s sources, Palestinian officials in Ramallah were devastated by news of the sudden cutoff of the main sources of the PA’s revenue. Even the Qatar ruler, whom Abbas visited last week as a last resort to save the PA from economic meltdown, refused to release any more funding.   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-02 01:19:41.756664   \n",
      "1  2018-02-02 01:19:41.756664   \n",
      "2  2018-02-02 01:19:41.756664   \n",
      "3  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                                                                                                                                                             title  \\\n",
      "0                                                                                       Is life an ILLUSION? Researchers prove 'reality doesn't exist if you're not looking at it'   \n",
      "1                                                                                                                                                                     Donald Trump   \n",
      "2                                                                                                                                                                     Donald Trump   \n",
      "3  MORE WINNING! Israeli intelligence source, DEBKAfile, confirms the Trump Administration to cut off ties with the Palestinians, which means no peace plan, no more financial aid   \n",
      "\n",
      "                                                                                                                                                authors  \\\n",
      "0                                                                                                                                           Sean Martin   \n",
      "1                                                             Linda Rivera, Conrad Calvano, Az Gal, Lincoln Applegate Hahn, Kevin Collins, Jesus Rivera   \n",
      "2                                                             Linda Rivera, Conrad Calvano, Az Gal, Lincoln Applegate Hahn, Kevin Collins, Jesus Rivera   \n",
      "3  Cleavis Nowell, Cleavisnowell, Clarence J. Feinour, Don Spilman, Jay Dillon, Al Dajjal, Linda Rivera, Conrad Calvano, Az Gal, Lincoln Applegate Hahn   \n",
      "\n",
      "   keywords meta_keywords  \\\n",
      "0       NaN          ['']   \n",
      "1       NaN          ['']   \n",
      "2       NaN          ['']   \n",
      "3       NaN          ['']   \n",
      "\n",
      "                                                                                                       meta_description  \\\n",
      "0  THE UNIVERSE ceases to exist when we are not looking at it proving that life is an illusion, according to one study.   \n",
      "1                                                                                                                   NaN   \n",
      "2                                                                                                                   NaN   \n",
      "3                                                                                                                   NaN   \n",
      "\n",
      "   tags  summary  source  \n",
      "0   NaN      NaN     NaN  \n",
      "1   NaN      NaN     NaN  \n",
      "2   NaN      NaN     NaN  \n",
      "3   NaN      NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Task 3\n",
    "#Big df\n",
    "csv_textfilereader = pd.read_csv('/Users/simonmork/Desktop/GitHub/FinalProjectDS/news_cleaned_2018_02_13.csv', chunksize=1000000)\n",
    "\n",
    "\n",
    "print(csv_textfilereader.get_chunk(4))\n",
    "\n",
    "\n",
    "#for i, current_df in enumerate(df):\n",
    "    #print(current_df)\n",
    "    #print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "092cc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Clean dataframe function\n",
    "def clean_df(df):\n",
    "        print(\"Getting headers..\")\n",
    "        column_names = list(df.columns.values)\n",
    "        #print(column_names)\n",
    "\n",
    "        print(\"Cleaning...\")\n",
    "        for index, row in df.iterrows():\n",
    "            #print(df.at[index,'content'])\n",
    "            cleaned = clean_text(row['content'])\n",
    "            df.at[index,'content'] = cleaned\n",
    "            #print(cleaned)\n",
    "            \n",
    "            \n",
    "        # Split the content column into a list of words\n",
    "        words = df['content'].str.split()\n",
    "        # Create a set of unique words\n",
    "        unique_words = set(word for word_list in words for word in word_list)\n",
    "        # Count the number of unique words\n",
    "        num_unique_words_after_clean = len(unique_words)\n",
    "        print(num_unique_words_after_clean) \n",
    "\n",
    "\n",
    "\n",
    "        print(\"Tokenizing and removing stopwords...\")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for index, row in df.iterrows():    \n",
    "            #print(df.at[index,'content'])\n",
    "            df.at[index,'content'] = nltk.word_tokenize(row['content'])\n",
    "            \n",
    "            filtered_sentence = []\n",
    "            for w in df.at[index,'content']:\n",
    "                if w not in stop_words:\n",
    "                    filtered_sentence.append(w)\n",
    "            \n",
    "            df.at[index,'content'] = filtered_sentence\n",
    "            #print(filtered_sentence)\n",
    "        \n",
    "\n",
    "        # concatenate all the lists into a single list of words\n",
    "        words = [word for content in df['content'] for word in content]\n",
    "        # get the set of unique words\n",
    "        unique_words = set(words)\n",
    "        # get the number of unique words\n",
    "        num_unique_words_after_stop = len(unique_words)\n",
    "        print(num_unique_words_after_stop)\n",
    "        print(1-num_unique_words_after_stop/num_unique_words_after_clean)\n",
    "            \n",
    "            \n",
    "\n",
    "        print(\"Stemming\")\n",
    "        snowball = SnowballStemmer(language='english')\n",
    "        for index, row in df.iterrows():   \n",
    "            stemmed = []\n",
    "            for word in df.at[index,'content']:\n",
    "                stemmed.append(snowball.stem(word))\n",
    "            df.at[index,'content'] = stemmed\n",
    "            #print(stemmed)\n",
    "\n",
    "\n",
    "        # concatenate all the lists into a single list of words\n",
    "        words = [word for content in df['content'] for word in content]\n",
    "        # get the set of unique words\n",
    "        unique_words = set(words)\n",
    "        # get the number of unique words\n",
    "        num_unique_words_after_stem = len(unique_words)\n",
    "        print(num_unique_words_after_stem)\n",
    "        print(1-num_unique_words_after_stem/num_unique_words_after_stop)\n",
    "\n",
    "\n",
    "        print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e1c3260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    \"…I have set before you life and death, blessing and cursing’\\n\\ntherefore choose life, that both you and your descendants may live.\"\\n\\n--Deuteronomy 30:19 (NKJV)\\n\\nOn the crisp, sunny, fall Columbus Day in 1999, organizers of the \"Say So\" march approached the steps of the U.S. Supreme Court. The marchers, who were predominantly black pastors and lay persons, concluded their three-day protest at the site of two monumental cases: the school desegregation Brown v. Board of Education (1954) and the pro-abortion Roe v. Wade \"rights\" in t he latterconverged in the declaration of Rev. Johnny M. Hunter, the march’s sponsor and national director of Life, Education and Resource Network (LEARN), the largest black pro-life organization.\\n\\n‘\"Civil rights’ doesn’t mean anything without a right to life!\" declared Hunter. He and the other marchers were protesting the disproportionately high number of abortions in the black community. The high number is no accident. Many Americansblack and whiteare unaware of Planned Parenthood founder Margaret Sanger’s Negro Project. Sanger created this program in 1939, after the organization changed its name from the American Birth Control League (ABCL) to the Birth Control Federation of America (BCFA).\\n\\nThe aim of the program was to restrictmany believe exterminatethe black population. Under the pretense of \"better health\" and \"family planning,\" Sanger cleverly implemented her plan. What’s more shocking is Sanger’s beguilement of black America’s créme de la crémethose prominent, well educated and well-to-dointo executing her scheme. Some within the black elite saw birth control as a means to attain economic empowerment, elevate the race and garner the respect of whites.\\n\\nThe Negro Project has had lasting repercussions in the black community: \"We have become victims of genocide by our own hands,\" cried Hunter at the \"Say So\" march.\\n\\nMalthusian Eugenics\\n\\nMargaret Sanger aligned herself with the eugenicists whose ideology prevailed in the early 20th century. Eugenicists strongly espoused racial supremacy and \"purtiy\",\" particularly of the \"Aryan\" race. Eugenicists hoped to purify the bloodlines and improve the race by encouraging the \"fit\" to reproduce and the \"unfit\" to restrict their reproduction. They sought to contain the \"inferior\" races through segregation, sterilization, birth control and abortion.\\n\\nSanger embraced Malthusian eugenics. Thomas Robert Malthus, a 19th century cleric and professor of political economy, believed a population time bomb threatened the existence of the human race. He viewed social problems such as poverty, deprivation and hunger as evidence of this \"population crisis.\" According to writer George Grant, Malthus condemned charities and other forms of benevolence, because he believed they only exacerbated the problems. His answer was to restrict population growth of certain groups of people. His theories of population growth and economic stability became the basis for national and international social policy. Grant quotes from Malthus’ magnum opus, An Essay on the Principle of Population, published in six editions from 1798 to 1826:\\n\\nAll children born, beyond what would be required to keep up the population to a desired level, must necessarily perish, unless room is made for them by the deaths of grown persons. We should facilitate, instead of foolishly and vainly endeavoring to impede, the operations of nature in producing this mortality.\\n\\nMalthus disciples believed if Western civilization were to survive, the physically unfit, the materially poor, the spiritually diseased, the racially inferior, and the mentally incompetent had to be suppressed and isolatedor even, perhaps, eliminated. His disciples felt the subtler and more \"scientific\" approaches of education, contraception, sterilization and abortion were more \"practical and acceptable ways\" to ease the pressures of the alleged overpopulation.\\n\\nCritics of Malthusianism said the group \"produced a new vocabulary of mumbo-jumbo. It was all hard-headed, scientific and relentless.\" Further, historical facts have proved the Malthusian mathematical scheme regarding overpopulation to be inaccurate, though many still believe them.\\n\\nDespite the falsehoods of Malthus’ overpopulation claims, Sanger nonetheless immersed herself in Malthusian eugenics. Grant wrote she argued for birth control using the \"scientifically verified\" threat of poverty, sickness, racial tension and overpopulation as its background. Sanger’s publication, The Birth Control Review (founded in 1917) regularly published pro-eugenic articles from eugenicists, such as Ernst Ruin. Although Sanger ceased editing The Birth Control Review in 1929, the ABCL continued to use it as a platform for eugenic ideas.\\n\\nSanger built the work of the ABCL, and, ultimately, Planned Parenthood, on the ideas and resources of the eugenics movement. Grant reported that \"virtually all of the organization’s board members were eugenicists.\" Eugenicists financed the early projects, from the opening of birth control clinics to the publishing of \"revolutionary\" literature. Eugenicists comprised the speakers at conferences, authors of literature and the providers of services \"almost without the exception.\" And Planned Parenthood’s international work was originally housed in the offices of the Eugenics Society. The two organizations were intertwined for years.\\n\\nThe ABCL became a legal entity on April 22, 1922, in New York. Before that, Sanger illegally operated a birth control clinic in October 1916, in the Brownsville section of Brooklyn, New York, which eventually closed. The clinic serviced the poor immigrants who heavily populated the areathose deemed \"unfit\" to reproduce.\\n\\nSanger’s early writings clearly reflected Malthus’ influence. She writes:\\n\\nOrganized charity itself is the symptom of a malignant social disease. Those vast, complex, interrelated organizations aiming to control and to diminish the spread of misery and destitution and all the menacing evils that spring out of this sinisterly fertile soil, are the surest sign that our civilization has bred, is breeding and perpetuating constantly increasing numbers of defectives, delinquents and dependents.\n",
      "Name: content, dtype: object\n",
      "Getting headers..\n",
      "Cleaning...\n",
      "488\n",
      "Tokenizing and removing stopwords...\n",
      "422\n",
      "0.13524590163934425\n",
      "Stemming\n",
      "382\n",
      "0.09478672985781988\n",
      "Done...\n",
      "12    [set, life, death, bless, curs, therefor, choos, life, descend, may, live, deuteronomi, numnum, nkjv, crisp, sunni, fall, columbus, day, num, organ, say, march, approach, step, us, suprem, court, marcher, predomin, black, pastor, lay, person, conclud, threeday, protest, site, two, monument, case, school, desegreg, brown, v, board, educ, num, proabort, roe, v, wade, right, latterconverg, declar, rev, johnni, hunter, march, sponsor, nation, director, life, educ, resourc, network, learn, largest, black, prolif, organ, civil, right, doesnt, mean, anyth, without, right, life, declar, hunter, marcher, protest, disproportion, high, number, abort, black, communiti, high, number, accid, mani, americansblack, whitear, unawar, plan, parenthood, founder, margaret, ...]\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#do it for first dataframe\n",
    "var = csv_textfilereader.get_chunk(1)\n",
    "print(var.content)\n",
    "clean_df(var)\n",
    "print(var['content'])\n",
    "df = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef1080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting number of urls, dates and num, and fake news and trump\n",
      "Num urls:  0\n",
      "Num num:  0\n",
      "Num trymp:  0\n",
      "Num fake:  155\n",
      "Num notfake:  95\n"
     ]
    }
   ],
   "source": [
    "#Part 1 - Task 2\n",
    "print(\"Counting number of urls, dates and num, and fake news and trump\")\n",
    "url = 0\n",
    "num = 0\n",
    "trymp=0\n",
    "fakenews = 0\n",
    "notfake = 0\n",
    "for index, row in df.iterrows():\n",
    "    #print(data['content'])\n",
    "    for word in df.at[index,'content']:\n",
    "        #print(word)\n",
    "        if word == 'url':\n",
    "            url = url+1\n",
    "        if word == 'num':\n",
    "            num = num+1\n",
    "        if word == 'trump':\n",
    "            trymp = trymp+1\n",
    "    if df.at[index,'type'] == 'fake':\n",
    "        fakenews = fakenews+1\n",
    "    else:\n",
    "        notfake = notfake+1\n",
    "print(\"Num urls: \", url)\n",
    "print(\"Num num: \", num)\n",
    "print(\"Num trymp: \", trymp)\n",
    "print(\"Num fake: \", fakenews)\n",
    "print(\"Num notfake: \", notfake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b09fd9d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r3/3zlyy5351w142fn8p5ldy2k00000gn/T/ipykernel_2071/20910180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(\"y: \", y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2421\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m     )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2098\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2099\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Part1 - Task 4\n",
    "\n",
    "X, y = df.content, df['type']\n",
    "\n",
    "#print(X)\n",
    "#print(\"y: \", y)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "print(\"train: \")\n",
    "print(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"test: \")\n",
    "print(X_test, y_test)\n",
    "\n",
    "print(\"Validation: \")\n",
    "print(X_val, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bin(to_bin):\n",
    "    to_bin = np.asarray(to_bin)\n",
    "    reliable = 0\n",
    "    fake = 0\n",
    "    bin_list = []\n",
    "    for i in range(len(to_bin)):\n",
    "        if to_bin[i] != 'fake':\n",
    "            bin_list.append(0)\n",
    "            reliable = reliable +1\n",
    "        else:\n",
    "            bin_list.append(1)\n",
    "            fake = fake+1\n",
    "    print(reliable)\n",
    "    print(fake)\n",
    "    return bin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8bbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "118\n",
      "y_train:  [0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "10\n",
      "15\n",
      "y_val:  [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1]\n",
      "3\n",
      "22\n",
      "y_test:  [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "y_train_bin = make_bin(y_train)\n",
    "print(\"y_train: \", y_train_bin)\n",
    "y_val_bin = make_bin(y_val)\n",
    "print(\"y_val: \", y_val_bin)\n",
    "y_test_bin = make_bin(y_test)\n",
    "print(\"y_test: \", y_test_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bda1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_vectoriser(vectee):\n",
    "    vectee = [' '.join(words) for words in vectee]\n",
    "    vectorizer = CountVectorizer(vocabulary=unique_words)\n",
    "    vectee = vectorizer.fit_transform(vectee)\n",
    "    return vectee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227baf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vec:    (0, 73)\t4\n",
      "  (0, 91)\t1\n",
      "  (0, 112)\t1\n",
      "  (0, 119)\t1\n",
      "  (0, 127)\t1\n",
      "  (0, 160)\t2\n",
      "  (0, 164)\t1\n",
      "  (0, 178)\t1\n",
      "  (0, 199)\t1\n",
      "  (0, 207)\t1\n",
      "  (0, 223)\t1\n",
      "  (0, 255)\t1\n",
      "  (0, 267)\t2\n",
      "  (0, 288)\t1\n",
      "  (0, 304)\t1\n",
      "  (0, 321)\t1\n",
      "  (0, 364)\t2\n",
      "  (0, 375)\t4\n",
      "  (0, 415)\t1\n",
      "  (0, 440)\t1\n",
      "  (0, 462)\t1\n",
      "  (0, 473)\t1\n",
      "  (0, 481)\t2\n",
      "  (0, 493)\t1\n",
      "  (0, 500)\t1\n",
      "  :\t:\n",
      "  (198, 10354)\t1\n",
      "  (198, 10386)\t1\n",
      "  (198, 10399)\t2\n",
      "  (198, 10469)\t1\n",
      "  (198, 10486)\t1\n",
      "  (198, 10547)\t1\n",
      "  (198, 10563)\t3\n",
      "  (198, 10601)\t1\n",
      "  (198, 10633)\t2\n",
      "  (198, 10650)\t1\n",
      "  (198, 10653)\t1\n",
      "  (198, 10733)\t1\n",
      "  (198, 10794)\t1\n",
      "  (198, 10888)\t5\n",
      "  (198, 10932)\t1\n",
      "  (198, 10944)\t1\n",
      "  (198, 10971)\t1\n",
      "  (199, 1659)\t1\n",
      "  (199, 2357)\t2\n",
      "  (199, 2612)\t1\n",
      "  (199, 2933)\t1\n",
      "  (199, 3083)\t1\n",
      "  (199, 5604)\t1\n",
      "  (199, 9807)\t1\n",
      "  (199, 10177)\t1\n",
      "X_val_vec:    (0, 368)\t1\n",
      "  (0, 603)\t1\n",
      "  (0, 952)\t2\n",
      "  (0, 1462)\t1\n",
      "  (0, 1529)\t1\n",
      "  (0, 1589)\t1\n",
      "  (0, 1632)\t1\n",
      "  (0, 1968)\t1\n",
      "  (0, 2033)\t1\n",
      "  (0, 2153)\t1\n",
      "  (0, 2169)\t1\n",
      "  (0, 2332)\t1\n",
      "  (0, 2337)\t1\n",
      "  (0, 2349)\t2\n",
      "  (0, 2678)\t2\n",
      "  (0, 2768)\t1\n",
      "  (0, 2827)\t2\n",
      "  (0, 2937)\t1\n",
      "  (0, 2967)\t1\n",
      "  (0, 3138)\t1\n",
      "  (0, 3314)\t1\n",
      "  (0, 3340)\t1\n",
      "  (0, 3664)\t7\n",
      "  (0, 3731)\t1\n",
      "  (0, 3773)\t5\n",
      "  :\t:\n",
      "  (24, 10076)\t4\n",
      "  (24, 10077)\t2\n",
      "  (24, 10092)\t1\n",
      "  (24, 10115)\t1\n",
      "  (24, 10122)\t1\n",
      "  (24, 10253)\t1\n",
      "  (24, 10354)\t1\n",
      "  (24, 10366)\t3\n",
      "  (24, 10513)\t1\n",
      "  (24, 10523)\t1\n",
      "  (24, 10532)\t1\n",
      "  (24, 10601)\t1\n",
      "  (24, 10603)\t1\n",
      "  (24, 10653)\t5\n",
      "  (24, 10654)\t1\n",
      "  (24, 10676)\t1\n",
      "  (24, 10698)\t1\n",
      "  (24, 10853)\t1\n",
      "  (24, 10888)\t1\n",
      "  (24, 10906)\t1\n",
      "  (24, 10944)\t2\n",
      "  (24, 10977)\t1\n",
      "  (24, 10982)\t3\n",
      "  (24, 10986)\t2\n",
      "  (24, 11006)\t1\n",
      "X_test_vec:    (0, 51)\t2\n",
      "  (0, 169)\t1\n",
      "  (0, 770)\t1\n",
      "  (0, 1040)\t1\n",
      "  (0, 1080)\t2\n",
      "  (0, 1326)\t6\n",
      "  (0, 1528)\t1\n",
      "  (0, 1532)\t2\n",
      "  (0, 2191)\t1\n",
      "  (0, 2208)\t1\n",
      "  (0, 2918)\t1\n",
      "  (0, 3275)\t1\n",
      "  (0, 3339)\t4\n",
      "  (0, 3380)\t1\n",
      "  (0, 3672)\t1\n",
      "  (0, 3773)\t1\n",
      "  (0, 3940)\t3\n",
      "  (0, 3963)\t1\n",
      "  (0, 3979)\t2\n",
      "  (0, 4150)\t1\n",
      "  (0, 4298)\t1\n",
      "  (0, 4377)\t1\n",
      "  (0, 5048)\t2\n",
      "  (0, 5119)\t1\n",
      "  (0, 5368)\t1\n",
      "  :\t:\n",
      "  (24, 9492)\t1\n",
      "  (24, 9574)\t2\n",
      "  (24, 9582)\t1\n",
      "  (24, 9760)\t1\n",
      "  (24, 9861)\t1\n",
      "  (24, 10017)\t1\n",
      "  (24, 10061)\t1\n",
      "  (24, 10076)\t1\n",
      "  (24, 10115)\t1\n",
      "  (24, 10125)\t1\n",
      "  (24, 10215)\t1\n",
      "  (24, 10243)\t1\n",
      "  (24, 10342)\t1\n",
      "  (24, 10366)\t1\n",
      "  (24, 10379)\t1\n",
      "  (24, 10399)\t2\n",
      "  (24, 10514)\t1\n",
      "  (24, 10547)\t1\n",
      "  (24, 10601)\t6\n",
      "  (24, 10680)\t1\n",
      "  (24, 10725)\t1\n",
      "  (24, 10731)\t2\n",
      "  (24, 10744)\t1\n",
      "  (24, 10859)\t1\n",
      "  (24, 10873)\t2\n"
     ]
    }
   ],
   "source": [
    "X_train_vec = X_vectoriser(X_train)\n",
    "print(\"X_train_vec: \", X_train_vec)\n",
    "X_val_vec = X_vectoriser(X_val)\n",
    "print(\"X_val_vec: \", X_val_vec)\n",
    "X_test_vec = X_vectoriser(X_test)\n",
    "print(\"X_test_vec: \", X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d92696",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/r3/3zlyy5351w142fn8p5ldy2k00000gn/T/ipykernel_2071/2866043434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msimple_log_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msimple_log_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_log_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_vec' is not defined"
     ]
    }
   ],
   "source": [
    "simple_log_model = LogisticRegression(max_iter=100000)\n",
    "simple_log_model.fit(X_train_vec, y_train_bin)\n",
    "y_pred = simple_log_model.predict(X_val_vec)\n",
    "\n",
    "print(accuracy_score(y_val_bin, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27842c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tips: nearest neighbor on missing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
