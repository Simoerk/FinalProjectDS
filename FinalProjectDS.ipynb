{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfa15bbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/simonmork/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting csv...\n",
      "Getting headers..\n",
      "Cleaning...\n",
      "1037899\n",
      "Tokenizing and removing stopwords...\n",
      "122049\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Calls the clean function from cleantext clean on a string\"\"\"\n",
    "    t = clean(text,\n",
    "    fix_unicode=False,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=True,         # fully strip line breaks as opposed to only normalizing them NOT WORKING?\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"URL\",\n",
    "    replace_with_email=\"EMAIL\",\n",
    "    replace_with_phone_number=\"PHONE\",\n",
    "    replace_with_number=\"NUMBER\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"CUR\",\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    "    )\n",
    "    return t\n",
    "\n",
    "\n",
    "print(\"Getting csv...\")\n",
    "pd.set_option(\"display.max_colwidth\", 10000000)\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv')\n",
    "\n",
    "\n",
    "print(\"Getting headers..\")\n",
    "column_names = list(data.columns.values)\n",
    "\n",
    "\n",
    "print(\"Cleaning...\")\n",
    "for index, row in data.iterrows():\n",
    "    cleaned = clean_text(row['content'])\n",
    "    data.at[index,'content'] = cleaned\n",
    "    \n",
    "\n",
    "len_before_process = sum(map(len, data.content))\n",
    "print(len_before_process)\n",
    "    \n",
    "    \n",
    "print(\"Tokenizing and removing stopwords...\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for index, row in data.iterrows():    \n",
    "    data.at[index,'content'] = nltk.word_tokenize(row['content'])\n",
    "    \n",
    "    filtered_sentence = []\n",
    "    for w in data.at[index,'content']:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    \n",
    "    data.at[index,'content'] = filtered_sentence\n",
    "\n",
    "    \n",
    "len_after_process = sum(map(len, data.content))\n",
    "print(len_after_process)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Done...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1080e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27842c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tips: nearest neighbor on missing data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
